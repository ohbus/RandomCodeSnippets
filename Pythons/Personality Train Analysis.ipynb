{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/subho/Softwares/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:455: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/subho/Softwares/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:456: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/subho/Softwares/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:457: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/subho/Softwares/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/subho/Softwares/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import decimal as dc\n",
    "from nltk.stem import PorterStemmer \n",
    "import statistics as st\n",
    "# define sequences\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import re\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import keras\n",
    "#import math\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "files=[dir for dir in os.walk('../input/transcripts')]\n",
    "vlogs=os.listdir(\"../input/transcript-data/transcripts/transcripts\")\n",
    "#print(vlogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mbti_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      type                                              posts\n",
      "0     INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "1     ENTP  'I'm finding the lack of me in these posts ver...\n",
      "2     INTP  'Good one  _____   https://www.youtube.com/wat...\n",
      "3     INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
      "4     ENTJ  'You're fired.|||That's another silly misconce...\n",
      "...    ...                                                ...\n",
      "8670  ISFP  'https://www.youtube.com/watch?v=t8edHB_h908||...\n",
      "8671  ENFP  'So...if this thread already exists someplace ...\n",
      "8672  INTP  'So many questions when i do these things.  I ...\n",
      "8673  INFP  'I am very conflicted right now when it comes ...\n",
      "8674  INFP  'It has been too long since I have been on per...\n",
      "\n",
      "[8675 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8675"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_text(num):\n",
    "    with open(\"../input/transcript-data/transcripts/transcripts/\"+vlogs[num])as f:\n",
    "        data=f.read().replace('\\t','').replace('\\n','')\n",
    "        ind=re.findall(r'\\d+',f.name)\n",
    "        if len(ind)==1:\n",
    "            return data,int(ind[0])\n",
    "        else:\n",
    "            print(\"Error:Multiple numbers are appeared as File Index\")\n",
    "      #function to do basic data cleaning takes a single argument Return Data                                       \n",
    "    \n",
    "    \n",
    "#data,index=fetch_text(2)\n",
    "#print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "FLOAT=re.compile('[-+]?\\d*\\.\\d+|\\d+')\n",
    "ps = PorterStemmer() \n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = FLOAT.sub('', text)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    \n",
    "    text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(ps.stem(word) for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speech_Data = np.array([])\n",
    "#indList=[]\n",
    "sp_data={}\n",
    "for seq in range(0,len(vlogs)):\n",
    "    data,index=fetch_text(seq)\n",
    "    data=clean_text(data)\n",
    "    #print(data)\n",
    "    #indList.append(index)\n",
    "    #Speech_Data = np.append(Speech_Data, data)\n",
    "    sp_data[index]=data\n",
    "sorted_data = collections.OrderedDict(sorted(sp_data.items()))\n",
    "data_val=list(sorted_data.values()) # Preparing input data for train and test\n",
    "#print(data_val[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2Bin(listOflst):\n",
    "    lst1=[]\n",
    "    lst1.append([lst[0] for lst in listOflst])\n",
    "    avg1=float(dc.Decimal(\"%.3f\"%np.mean(lst1)))\n",
    "    lst1=[]\n",
    "    lst1.append([lst[1] for lst in listOflst])\n",
    "    avg2=float(dc.Decimal(\"%.3f\"%np.mean(lst1)))\n",
    "    lst1=[]\n",
    "    lst1.append([lst[2] for lst in listOflst])\n",
    "    avg3=float(dc.Decimal(\"%.3f\"%np.mean(lst1)))\n",
    "    lst1=[]\n",
    "    lst1.append([lst[3] for lst in listOflst])\n",
    "    avg4=float(dc.Decimal(\"%.3f\"%np.mean(lst1)))\n",
    "    #print(avg4)\n",
    "    bin_Score=[]\n",
    "    for lst in listOflst:\n",
    "        bn_lst=[]\n",
    "        if lst[0]>avg1:\n",
    "            bn_lst.append(1)\n",
    "        else:\n",
    "            bn_lst.append(0)\n",
    "            \n",
    "        if lst[1]>10:\n",
    "        #if lst[1]>avg2:\n",
    "            bn_lst.append(1)\n",
    "        else:\n",
    "            bn_lst.append(0)\n",
    "\n",
    "        if lst[2]>10:\n",
    "        #if lst[1]>avg3:\n",
    "            bn_lst.append(1)\n",
    "        else:\n",
    "            bn_lst.append(0)\n",
    "\n",
    "        if lst[3]>10:\n",
    "        #if lst[1]>avg4:\n",
    "            bn_lst.append(1)\n",
    "        else:\n",
    "            bn_lst.append(0)\n",
    "        bin_Score.append(bn_lst)\n",
    "    return bin_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_score=[]\n",
    "#Preparing data for label\n",
    "with open(\"../input/personality-score/scores.csv\")as f:\n",
    "    data=f.read()\n",
    "    data_pro=data.splitlines()\n",
    "    for i in range(1,len(data_pro)):\n",
    "        d_score=data_pro[i].split()\n",
    "        ds= map(float,d_score[1:])\n",
    "        data_sc = [float(dc.Decimal(\"%.3f\" % e)) for e in ds] # Gold Standard score on each trait for all subject\n",
    "        #data_sc = [1 if e>=4.6 else e*0 for e in ds]\n",
    "        #inx=re.findall(r'\\d+',d_score[0])\n",
    "        ind_score.append(data_sc[1:])\n",
    "        #print(ind_score)\n",
    "        \n",
    "ind_score=Conv2Bin(ind_score)       \n",
    "        \n",
    "print(ind_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 400\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(data_val)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(data_val)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fscore=pd.DataFrame(ind_score)\n",
    "Y = pd.get_dummies(fscore).values\n",
    "#Y = np.ndarray(ind_score) \n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(keras.layers.SpatialDropout1D(0.2))\n",
    "model.add(keras.layers.recurrent.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 33\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1\n",
    "                    ,callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.00001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['acc'], label='train')\n",
    "plt.plot(history.history['val_acc'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
